{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.225985Z",
     "start_time": "2024-10-01T15:51:11.223079Z"
    }
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import pathlib\n",
    "import pickle\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specification of data and supplementary dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.273683Z",
     "start_time": "2024-10-01T15:51:11.268107Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_type = '2009-2010'\n",
    "data_type = '2000-2001'\n",
    "# data_type = '1999-2000'\n",
    "# data_type = 'subsample'\n",
    "DATA_PATH = f'../data arxiv/hyperedges_list_arxiv_{data_type}.txt'\n",
    "LINEGRAPH_EDGES_PATH = f'../suplementary/linegraphs/linegraph_weighted_edges_list_{data_type}.txt'\n",
    "LINEGRAPH_NODES_PATH = f'../suplementary/linegraphs/linegraph_weighted_nodes_list_{data_type}.txt'\n",
    "DIST_DIR = \"../suplementary/dist\"\n",
    "CLIQUE_PATH = f\"../suplementary/clique-projections/{data_type}-proj-graph.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.306185Z",
     "start_time": "2024-10-01T15:51:11.301536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8914\n",
      "'hep-ex', 'physics.comp-ph'\n",
      "'math.ag', 'math.cv'\n"
     ]
    }
   ],
   "source": [
    "data_path = DATA_PATH\n",
    "with open(data_path) as file:\n",
    "    line = file.readline()\n",
    "\n",
    "if data_type in [\"subsample\", \"test\"]:\n",
    "    hyperedges_list = line.split('], ')\n",
    "    hyperedges_list = [x.replace('[','') for x in hyperedges_list]\n",
    "    hyperedges_list = [x.replace(']','') for x in hyperedges_list] # removes the last \"]\"\"\n",
    "else:\n",
    "    hyperedges_list = line.split(']\", ')\n",
    "    hyperedges_list = [x.replace('\"[','') for x in hyperedges_list]\n",
    "    hyperedges_list = [x.replace(']\"','') for x in hyperedges_list] # removes the last \"]\"\"\n",
    "    \n",
    "print(len(hyperedges_list))\n",
    "print(hyperedges_list[0])\n",
    "print(hyperedges_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.319331Z",
     "start_time": "2024-10-01T15:51:11.307033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2205\n",
      "2205\n",
      "[['astro-ph', 'astro-ph.ga'], ['astro-ph', 'cond-mat'], ['astro-ph', 'cond-mat', 'gr-qc', 'hep-ph']]\n",
      "[1 1 2]\n"
     ]
    }
   ],
   "source": [
    "hyperedges_list, counts_list = np.unique(hyperedges_list, return_counts = True) # counts_list contains info about frequencies of each hyperedge\n",
    "print(len(hyperedges_list))\n",
    "print(len(counts_list))\n",
    "\n",
    "hyperedges_list = [x.replace(\"'\",'') for x in hyperedges_list]\n",
    "hyperedges_list = [x.split(', ') for x in hyperedges_list]\n",
    "print(hyperedges_list[:3])\n",
    "print(counts_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.323086Z",
     "start_time": "2024-10-01T15:51:11.320272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2205\n"
     ]
    }
   ],
   "source": [
    "# left hyperedges of size 1\n",
    "hyperedges_list_ids = [i for i in range(len(hyperedges_list)) if len(hyperedges_list[i]) > 1]\n",
    "print(len(hyperedges_list_ids))\n",
    "if len(hyperedges_list_ids) < len(hyperedges_list):\n",
    "    hyperedges_list = [hyperedges_list[i] for i in hyperedges_list_ids]\n",
    "    counts_list = [counts_list[i] for i in hyperedges_list_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.325526Z",
     "start_time": "2024-10-01T15:51:11.323631Z"
    }
   },
   "outputs": [],
   "source": [
    "# create supplementary dictionary containing nodes as keys and hyperedges to which they are incident as values. It helps to access needed hyperedges faster\n",
    "def get_nodes_to_edges(edges_to_nodes_dict):\n",
    "    nodes_to_edges_dict = {}\n",
    "    for edge, nodes in edges_to_nodes_dict.items():\n",
    "        for node in nodes:\n",
    "            if node in nodes_to_edges_dict.keys():\n",
    "                nodes_to_edges_dict[node].append(edge)\n",
    "            else:\n",
    "                nodes_to_edges_dict[node] = [edge]\n",
    "    return nodes_to_edges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.330353Z",
     "start_time": "2024-10-01T15:51:11.326187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes count = 135\n"
     ]
    }
   ],
   "source": [
    "edges_to_nodes_dict = {i: hyperedges_list[i] for i in range(len(hyperedges_list))} # key = hyperedge, value = nodes in hyperedge\n",
    "edges_to_counts_dict = {i: counts_list[i] for i in range(len(counts_list))}\n",
    "nodes_to_edges_dict = get_nodes_to_edges(edges_to_nodes_dict) # key = node, value = list of hyperedges in which the node participates\n",
    "print(f'nodes count = {len(nodes_to_edges_dict.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-graph construction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.339582Z",
     "start_time": "2024-10-01T15:51:11.332019Z"
    }
   },
   "outputs": [],
   "source": [
    "# weight_function is needed for creation of weighted line graph, w1 (w2) is e1's (e2's) weight in weighted hypergraph\n",
    "def weight_function(e1, e2, w1, w2):\n",
    "    if e1 == e2:\n",
    "        return (1 / w1) * ((1 / 3) * (len(e1) + 1) - 1)\n",
    "    union_len = len(set(e1).union(e2))\n",
    "    intersect_len = len(set(e1).intersection(e2))\n",
    "    return (1 / 2) * (1 / w1 + 1 / w2) *((1 / 3) * union_len * (1 + 1 / intersect_len) - 1)\n",
    "\n",
    "\n",
    "def generate_weighted_line_graph(edges_to_nodes_dict, edges_to_counts_dict):\n",
    "    file_path = pathlib.Path(LINEGRAPH_EDGES_PATH)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "\n",
    "        items = list(edges_to_nodes_dict.items())\n",
    "        line_nodes_ids_dict = {items[i][0]: i for i in range(len(items))} # in line-graph nodes = hyperedges in intial hypergraph\n",
    "        line_edges_list = [] \n",
    "        line_edges_weights_list = []\n",
    "\n",
    "        for i in range(len(items)):\n",
    "            edge, nodes = items[i]    \n",
    "            line_edges_list.append([i,i]) # our line-graph has self-loops\n",
    "            line_edges_weights_list.append(weight_function(nodes, nodes, \n",
    "                                                           edges_to_counts_dict[edge], edges_to_counts_dict[edge])) # add weights of self-loops\n",
    "\n",
    "        for i in tqdm.tqdm(range(len(items))):    \n",
    "            edge, nodes = items[i]\n",
    "            set_nodes = set(nodes)\n",
    "            neighbors = [k for k,v in items if \n",
    "                         not k == edge and\n",
    "                         (len(v) >= len(nodes)) and \n",
    "                         not set_nodes.isdisjoint(v)] # find hyperedges that intersect with the current one\n",
    "            \n",
    "            if len(neighbors) > 0:\n",
    "                for neighbor in neighbors:\n",
    "                    line_edges_list.append([line_nodes_ids_dict[edge],line_nodes_ids_dict[neighbor]]) # each intersection leads to edge in the line-graph\n",
    "                    line_edges_weights_list.append(weight_function(nodes, edges_to_nodes_dict[neighbor], \n",
    "                                                                   edges_to_counts_dict[edge], edges_to_counts_dict[neighbor])) \n",
    "        \n",
    "        G = ig.Graph(n=len(items), edges=line_edges_list, \n",
    "                        edge_attrs={'weight': line_edges_weights_list},\n",
    "                        vertex_attrs={'label': list(line_nodes_ids_dict.keys())})\n",
    "        edges_list = G.get_edgelist()\n",
    "        print('saving line graph')\n",
    "        with open(LINEGRAPH_EDGES_PATH,'w') as file:\n",
    "            lines = []\n",
    "            for edge_id in range(len(edges_list)):\n",
    "                edge_weight = G.es[edge_id][\"weight\"]\n",
    "                lines.append(f'{edges_list[edge_id][0]} {edges_list[edge_id][1]} {edge_weight}\\n')\n",
    "            file.writelines(lines)\n",
    "        with open(LINEGRAPH_NODES_PATH ,'w') as file:\n",
    "            file.writelines([str(G.vs[i]['label'])+'\\n' for i in range(len(items))])\n",
    "        \n",
    "        print('line graph saved')\n",
    "        del G\n",
    "    \n",
    "    with open(LINEGRAPH_EDGES_PATH,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    with open(LINEGRAPH_NODES_PATH,'r') as file:\n",
    "        lines_nodes = file.readlines()\n",
    "    edges_list = []\n",
    "    weights = []\n",
    "    nodes = []\n",
    "    print('preparing data for line graph creation')\n",
    "    print(len(lines))\n",
    "    for line in lines:\n",
    "        from_node,to_node,weight = line.split(' ')\n",
    "        from_node = int(from_node)\n",
    "        to_node = int(to_node)\n",
    "        if from_node not in nodes:\n",
    "            nodes.append(from_node)\n",
    "        if to_node not in nodes:\n",
    "            nodes.append(to_node)\n",
    "        weight = float(weight)\n",
    "        edges_list.append((from_node,to_node))\n",
    "        weights.append(weight)\n",
    "\n",
    "    print('line graph creation')\n",
    "    G = ig.Graph(n = len(nodes), edges=edges_list, \n",
    "                            edge_attrs={'weight': weights},\n",
    "                            vertex_attrs={'label': [int(x) for x in lines_nodes]})\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypergraph distance calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.351507Z",
     "start_time": "2024-10-01T15:51:11.342325Z"
    }
   },
   "outputs": [],
   "source": [
    "# to find shortest path from node u to node v one should, first, find path in line-graph from all nodes, representing hyperedges to which u belongs, to all nodes, representing all hyperedges to which v belongs \n",
    "# each hyperedge contains sereis of nodes, therefore,  paths between nodes in the line-graph might be the same for different node pairs.\n",
    "# To fasten calculation we can save shortest paths that we have already found  \n",
    "# global_all_path_dict contains information about previously found paths between nodes in line-graph (representing hyperedges in hypergraph)\n",
    "\n",
    "def get_distance_new(nodes_to_edges_dict, edges_to_counts_dict, line_graph, u, v, global_all_path_dict = {}, return_path = False):\n",
    "    from_edges = nodes_to_edges_dict[u] # from_edges - edges incident to node u\n",
    "    to_edges = nodes_to_edges_dict[v] # to_edges - edges incident to node v \n",
    "\n",
    "    if len(from_edges) < len(to_edges):\n",
    "        u_edges = from_edges\n",
    "        v_edges = to_edges\n",
    "    else:\n",
    "        u_edges = to_edges\n",
    "        v_edges = from_edges\n",
    "\n",
    "    line_nodes_ids_dict = {line_graph.vs[\"label\"][i]: i for i in range(len(line_graph.vs[\"label\"]))} \n",
    "    line_ids_nodes_dict = {i : line_graph.vs[\"label\"][i] for i in range(len(line_graph.vs[\"label\"]))}\n",
    "\n",
    "    uv_dist = []\n",
    "    uv_paths = []\n",
    "    \n",
    "    for u_e in u_edges:\n",
    "        if u_e in global_all_path_dict.keys():\n",
    "            all_paths_dict = global_all_path_dict[u_e].copy()\n",
    "        else:\n",
    "            all_paths_dict = {}\n",
    "        to_linegraph_nodes = [v_e for v_e in v_edges if v_e not in all_paths_dict.keys() and not v_e == u_e] # nodes in line-graph to which paths from u_e have not been found previously \n",
    "\n",
    "        drop_ve = []\n",
    "        for v_e in to_linegraph_nodes:\n",
    "            if v_e in global_all_path_dict.keys(): # paths are not directed. If we already have path from v_e to u_e it is the same path as from u_e to v_e and we should not find it once more\n",
    "                if u_e in global_all_path_dict[v_e].keys():\n",
    "                    all_paths_dict[v_e] = [list(np.flip(x)) for x in global_all_path_dict[v_e][u_e]]\n",
    "                    drop_ve.append(v_e)\n",
    "\n",
    "        all_paths = line_graph.get_all_shortest_paths(line_nodes_ids_dict[u_e], \n",
    "                                                      to = [line_nodes_ids_dict[x] for x in to_linegraph_nodes if x not in drop_ve], \n",
    "                                                      weights=line_graph.es[\"weight\"]) # calculation of absent shortest paths \n",
    "        \n",
    "        for i in range(len(all_paths)): # collecting of the new shortest paths\n",
    "            if len(all_paths[i]) > 1:\n",
    "                if not line_ids_nodes_dict[all_paths[i][-1]] in all_paths_dict.keys():\n",
    "                    all_paths_dict[line_ids_nodes_dict[all_paths[i][-1]]] = [all_paths[i]]\n",
    "\n",
    "                    if not line_ids_nodes_dict[all_paths[i][-1]] in global_all_path_dict.keys():\n",
    "                        global_all_path_dict[line_ids_nodes_dict[all_paths[i][-1]]] = {u_e : [list(np.flip(all_paths[i]))]}\n",
    "                    else:\n",
    "                        global_all_path_dict[line_ids_nodes_dict[all_paths[i][-1]]][u_e] = [list(np.flip(all_paths[i]))]\n",
    "                else:\n",
    "                    all_paths_dict[line_ids_nodes_dict[all_paths[i][-1]]].append(all_paths[i])\n",
    "                    global_all_path_dict[line_ids_nodes_dict[all_paths[i][-1]]][u_e].append(list(np.flip(all_paths[i])))\n",
    "        \n",
    "        global_all_path_dict[u_e] = all_paths_dict.copy() # saving of the renewed collection of the shortest paths from u_e\n",
    "        \n",
    "        for v_e in v_edges: # calculate distances between u and v corresponding to every u_e -> v_e \n",
    "            if u_e == v_e:\n",
    "                dist = line_graph.es[\"weight\"][line_graph.get_eid(line_nodes_ids_dict[u_e],line_nodes_ids_dict[v_e])]\n",
    "                uv_dist.append(dist + 1 / edges_to_counts_dict[u_e]) # modification for weighted hypergraph\n",
    "                uv_paths.append([line_graph.get_eid(line_nodes_ids_dict[u_e],line_nodes_ids_dict[v_e])])\n",
    "            else:\n",
    "                paths = all_paths_dict[v_e]\n",
    "                uv_paths.append(paths)\n",
    "                if len(paths[0]) > 0:\n",
    "                    distance = 0\n",
    "                for i in range (len(paths[0]) - 1):\n",
    "                    start = paths[0][i]\n",
    "                    end = paths[0][i + 1]\n",
    "                    e = line_graph.get_eid(start, end)\n",
    "                    distance += line_graph.es[e][\"weight\"]\n",
    "                    # uv_paths[-1].append((start,end))\n",
    "                distance += (1 / 2) * (1 / edges_to_counts_dict[u_e] + 1 / edges_to_counts_dict[v_e])\n",
    "                uv_dist.append(distance)\n",
    "    \n",
    "    if not return_path:\n",
    "        return min(uv_dist), global_all_path_dict\n",
    "    \n",
    "    min_indexes = [i for i in range(len(uv_dist)) if uv_dist[i] == min(uv_dist)] # indexes corresponding to minimal distance between differen u_e and v_e pairs\n",
    "\n",
    "    paths = []\n",
    "    for i in min_indexes:\n",
    "        if len(uv_paths[i]) > 1:\n",
    "            paths_str = [\",\".join([str(y) for y in x]) for x in uv_paths[i] ]\n",
    "            paths_str = np.unique(paths_str)\n",
    "            paths_tmp = [x.split(',') for x in paths_str]\n",
    "            paths_tmp = [[int(y) for y in x] for x in paths_tmp]\n",
    "            paths.extend(paths_tmp)\n",
    "        else:\n",
    "            paths.extend(uv_paths[i].copy())\n",
    "\n",
    "    return min(uv_dist), paths, global_all_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:11.354290Z",
     "start_time": "2024-10-01T15:51:11.352241Z"
    }
   },
   "outputs": [],
   "source": [
    "# suplementary function to calculate particulare path length\n",
    "def calc_path_weight(edges_to_counts_dict, line_graph, path):\n",
    "    line_ids_nodes_dict = {i : line_graph.vs[\"label\"][i] for i in range(len(line_graph.vs[\"label\"]))}\n",
    "    length = 0\n",
    "    for i in range (len(path) - 1):\n",
    "        start = path[i]\n",
    "        end = path[i + 1]\n",
    "        e = line_graph.get_eid(start, end)\n",
    "        length += line_graph.es[e][\"weight\"]\n",
    "    length += (1 / 2) * (1 / edges_to_counts_dict[line_ids_nodes_dict[path[0]]] +\\\n",
    "                          1 / edges_to_counts_dict[line_ids_nodes_dict[path[-1]]])\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:14.616744Z",
     "start_time": "2024-10-01T15:51:11.355627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data for line graph creation\n",
      "539052\n",
      "line graph creation\n"
     ]
    }
   ],
   "source": [
    "# if we constructed line-graph once it is saved into suplementary/linegraphs to fasten further calculation\n",
    "line_graph = generate_weighted_line_graph(edges_to_nodes_dict, edges_to_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of distances between specified pairs of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T16:35:45.555028Z",
     "start_time": "2024-10-01T15:52:16.048154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [08:09<00:00,  3.63s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 489.39867305755615\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [00:05<00:00, 26.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 5.027559757232666\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [07:19<00:00,  3.25s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 439.1307384967804\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [06:34<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 394.16274523735046\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [12:44<00:00,  5.66s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 764.0963671207428\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [06:43<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 403.2914810180664\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 52/135 [01:54<03:02,  2.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m tqdm\u001B[38;5;241m.\u001B[39mtqdm(nodes_to_edges_dict\u001B[38;5;241m.\u001B[39mkeys()):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m node \u001B[38;5;241m==\u001B[39m node_from:\n\u001B[0;32m---> 22\u001B[0m         dist, paths_tmp, global_all_path_dict \u001B[38;5;241m=\u001B[39m \u001B[43mget_distance_new\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnodes_to_edges_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medges_to_counts_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode_from\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglobal_all_path_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m         dists\u001B[38;5;241m.\u001B[39mappend(dist)\n\u001B[1;32m     24\u001B[0m         paths\u001B[38;5;241m.\u001B[39mappend(paths_tmp)\n",
      "Cell \u001B[0;32mIn[46], line 37\u001B[0m, in \u001B[0;36mget_distance_new\u001B[0;34m(nodes_to_edges_dict, edges_to_counts_dict, line_graph, u, v, global_all_path_dict, return_path)\u001B[0m\n\u001B[1;32m     34\u001B[0m             all_paths_dict[v_e] \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mlist\u001B[39m(np\u001B[38;5;241m.\u001B[39mflip(x)) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m global_all_path_dict[v_e][u_e]]\n\u001B[1;32m     35\u001B[0m             drop_ve\u001B[38;5;241m.\u001B[39mappend(v_e)\n\u001B[0;32m---> 37\u001B[0m all_paths \u001B[38;5;241m=\u001B[39m \u001B[43mline_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_all_shortest_paths\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline_nodes_ids_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mu_e\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mto\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mline_nodes_ids_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mto_linegraph_nodes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdrop_ve\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mline_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# calculation of absent shortest paths \u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(all_paths)): \u001B[38;5;66;03m# collecting of the new shortest paths\u001B[39;00m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(all_paths[i]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "dists_dict = {}\n",
    "paths_dict = {}\n",
    "node_names = list(nodes_to_edges_dict.keys())\n",
    "start_id = 0\n",
    "final_id = len(node_names) - 1\n",
    "# start_id = 120\n",
    "# final_id = 120\n",
    "file_path = pathlib.Path(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle')\n",
    "\n",
    "global_all_path_dict = {}\n",
    "    \n",
    "if True:\n",
    "    for id_from in range(start_id,final_id + 1):\n",
    "        time_start = time.time()\n",
    "\n",
    "        node_from = node_names[id_from]\n",
    "        dists = []\n",
    "        paths = []\n",
    "        print(id_from)\n",
    "        for node in tqdm.tqdm(nodes_to_edges_dict.keys()):\n",
    "            if not node == node_from:\n",
    "                dist, paths_tmp, global_all_path_dict = get_distance_new(nodes_to_edges_dict, edges_to_counts_dict, line_graph, node_from, node, global_all_path_dict, return_path=True)\n",
    "                dists.append(dist)\n",
    "                paths.append(paths_tmp)\n",
    "            else:\n",
    "                dists.append(0)\n",
    "                paths.append([])\n",
    "        dists_dict[node_from] = dists.copy()\n",
    "        paths_dict[node_from] = paths.copy()\n",
    "        with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle', 'wb') as handle: # we save calculated distances and corresponding shortest paths to fasten furter analysis\n",
    "                pickle.dump((dists_dict, paths_dict), handle)\n",
    "        time_term = time.time()\n",
    "        print(f'Elapsed time {time_term - time_start}')\n",
    "else:\n",
    "    with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle', 'rb') as handle:\n",
    "        dists_dict, paths_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T15:51:14.670837Z",
     "start_time": "2024-10-01T15:51:14.625052Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# data frame containing calculated distances between every nodes pair\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m hyper_dist_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdists_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m hyper_dist_df\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(nodes_to_edges_dict\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[1;32m      4\u001B[0m hyper_dist_df\n",
      "File \u001B[0;32m~/programming/hypergraph_distances/venv/lib/python3.10/site-packages/pandas/core/frame.py:778\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[0;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[1;32m    772\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_mgr(\n\u001B[1;32m    773\u001B[0m         data, axes\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m: index, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: columns}, dtype\u001B[38;5;241m=\u001B[39mdtype, copy\u001B[38;5;241m=\u001B[39mcopy\n\u001B[1;32m    774\u001B[0m     )\n\u001B[1;32m    776\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001B[39;00m\n\u001B[0;32m--> 778\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m \u001B[43mdict_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmanager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    779\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ma\u001B[38;5;241m.\u001B[39mMaskedArray):\n\u001B[1;32m    780\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mma\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mrecords\n",
      "File \u001B[0;32m~/programming/hypergraph_distances/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001B[0m, in \u001B[0;36mdict_to_mgr\u001B[0;34m(data, index, columns, dtype, typ, copy)\u001B[0m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    500\u001B[0m         \u001B[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001B[39;00m\n\u001B[1;32m    501\u001B[0m         arrays \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m arrays]\n\u001B[0;32m--> 503\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43marrays_to_mgr\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtyp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconsolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/programming/hypergraph_distances/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001B[0m, in \u001B[0;36marrays_to_mgr\u001B[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verify_integrity:\n\u001B[1;32m    112\u001B[0m     \u001B[38;5;66;03m# figure out the index, if necessary\u001B[39;00m\n\u001B[1;32m    113\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 114\u001B[0m         index \u001B[38;5;241m=\u001B[39m \u001B[43m_extract_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43marrays\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    116\u001B[0m         index \u001B[38;5;241m=\u001B[39m ensure_index(index)\n",
      "File \u001B[0;32m~/programming/hypergraph_distances/venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001B[0m, in \u001B[0;36m_extract_index\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    675\u001B[0m lengths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(raw_lengths))\n\u001B[1;32m    676\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(lengths) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 677\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAll arrays must be of the same length\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    679\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m have_dicts:\n\u001B[1;32m    680\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    681\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    682\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# data frame containing calculated distances between every nodes pair\n",
    "hyper_dist_df = pd.DataFrame(dists_dict)\n",
    "hyper_dist_df.index = list(nodes_to_edges_dict.keys())\n",
    "hyper_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted clique projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generating weighted clique projection\n",
    "# weights = 1 / frequency of appearance\n",
    "def make_clique_projection_data(edges_to_nodes_dict, edges_to_counts_dict):\n",
    "    file_path = pathlib.Path(CLIQUE_PATH)\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        proj_edges_dict = {}\n",
    "        for edge, nodes in edges_to_nodes_dict.items():\n",
    "            for i in range(len(nodes)-1):\n",
    "                for j in range(i+1, len(nodes)):\n",
    "                    node_from = nodes[i]\n",
    "                    node_to = nodes[j]\n",
    "                    if not (((node_from, node_to) in proj_edges_dict.keys()) \n",
    "                            or ((node_to, node_from) in proj_edges_dict.keys())):\n",
    "                        proj_edges_dict[(node_from,node_to)] = edges_to_counts_dict[edge]\n",
    "                    else:\n",
    "                        if ((node_from, node_to) in proj_edges_dict.keys()):\n",
    "                            proj_edges_dict[(node_from,node_to)] += edges_to_counts_dict[edge]\n",
    "                        else:\n",
    "                            proj_edges_dict[(node_to,node_from)] += edges_to_counts_dict[edge]\n",
    "        \n",
    "        with open(CLIQUE_PATH, 'w') as handle:\n",
    "            for (node_from, node_to), weight in proj_edges_dict.items():\n",
    "                handle.write(f'{node_from} {node_to} {weight}')\n",
    "                handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create cluque projection if it wasn't created previously\n",
    "make_clique_projection_data(edges_to_nodes_dict, edges_to_counts_dict)\n",
    "\n",
    "# load and clique projection\n",
    "proj_graph_file = CLIQUE_PATH\n",
    "with open(proj_graph_file) as file:\n",
    "        lines = file.readlines()\n",
    "        edges_list = []\n",
    "        weights_list = []\n",
    "        nodes_dict = {}\n",
    "        i = -1\n",
    "        for line in lines:\n",
    "            from_node, to_node, weight = line.split(\" \")\n",
    "            \n",
    "            # from_node = int(from_node)\n",
    "            # to_node = int(to_node)\n",
    "     \n",
    "            if not from_node in nodes_dict.keys():\n",
    "                i += 1\n",
    "                from_id = i\n",
    "                nodes_dict[from_node] = i\n",
    "            else:\n",
    "                from_id = nodes_dict[from_node]\n",
    "            if not to_node in nodes_dict.keys():\n",
    "                i += 1\n",
    "                to_id = i\n",
    "                nodes_dict[to_node] = i\n",
    "            else:\n",
    "                to_id = nodes_dict[to_node]\n",
    "            edges_list.append([from_id, to_id])\n",
    "            weights_list.append(1 / float(weight))\n",
    "\n",
    "proj_graph = ig.Graph(edges_list)\n",
    "proj_graph.es[\"weight\"] = weights_list\n",
    "\n",
    "proj_dist_dict = {}\n",
    "proj_dist_path = {}\n",
    "\n",
    "# calculation of distances and paths in cluque projection\n",
    "\n",
    "for from_node in dists_dict.keys():\n",
    "    from_node_id = nodes_dict[from_node]\n",
    "    dists = []\n",
    "    paths_tmp = []\n",
    "    for node in list(nodes_to_edges_dict.keys()):\n",
    "        node_id = nodes_dict[node]\n",
    "        paths = proj_graph.get_shortest_paths(from_node_id, node_id, weights=proj_graph.es[\"weight\"], output = \"epath\")\n",
    "        paths_tmp.append(paths[0])\n",
    "        distance = 0\n",
    "        for e in paths[0]:\n",
    "            distance += proj_graph.es[e][\"weight\"]\n",
    "        dists.append(distance)\n",
    "\n",
    "    proj_dist_dict[from_node] = dists\n",
    "    proj_dist_path[from_node] = paths_tmp\n",
    "\n",
    "\n",
    "# data frame containing dists in clique proj for every nodes pair\n",
    "proj_dist_df = pd.DataFrame(proj_dist_dict)\n",
    "proj_dist_df.index = list(nodes_to_edges_dict.keys())\n",
    "proj_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dist_df.to_csv(f\"results_hypergraph_dist_{data_type}.csv\")\n",
    "proj_dist_df.to_csv(f\"results_clique-proj_dist_{data_type}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hg_dists = []\n",
    "all_proj_dists = []\n",
    "\n",
    "for i in range(len(node_names)-1):\n",
    "    for j in range(i + 1,len(node_names)):\n",
    "        from_node = node_names[i]\n",
    "        to_node = node_names[j]\n",
    "        if not from_node == to_node:\n",
    "            all_hg_dists.append(dists_dict[from_node][j])\n",
    "            all_proj_dists.append(proj_dist_dict[from_node][j])\n",
    "max_q = 10\n",
    "hg_quantiles = [np.quantile(all_hg_dists, q/max_q) for q in range(1,max_q)]\n",
    "proj_quantiles = [np.quantile(all_proj_dists, q/max_q) for q in range(1,max_q)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dict = {\n",
    "    \"from node id\":[],\n",
    "    \"to node id\":[],\n",
    "    \"from node\":[],\n",
    "    \"to node\":[],\n",
    "    \"hypergraph distance\":[],\n",
    "    \"projected distance\":[],\n",
    "    \"diff\":[],\n",
    "    \"hypergraph rank\":[],\n",
    "    \"projected rank\":[],\n",
    "    \"rank difference\": []\n",
    "}\n",
    "for i in range(len(node_names)-1):\n",
    "    for j in range(i + 1,len(node_names)):\n",
    "        from_node = node_names[i]\n",
    "        to_node = node_names[j]\n",
    "        hg_dist = dists_dict[from_node][j]\n",
    "        proj_dist = proj_dist_dict[from_node][j]\n",
    "        \n",
    "        if hg_dist <= hg_quantiles[0]:\n",
    "            compare_dict[\"hypergraph rank\"].append(1)\n",
    "        elif hg_dist > hg_quantiles[-1]:\n",
    "            compare_dict[\"hypergraph rank\"].append(max_q)\n",
    "        else:\n",
    "            for q in range(1,max_q - 1):\n",
    "                if hg_dist > hg_quantiles[q-1] and hg_dist <= hg_quantiles[q]:\n",
    "                    compare_dict[\"hypergraph rank\"].append(q+1) \n",
    "\n",
    "        if proj_dist <= proj_quantiles[0]:\n",
    "            compare_dict[\"projected rank\"].append(1)\n",
    "        elif proj_dist > proj_quantiles[-1]:\n",
    "            compare_dict[\"projected rank\"].append(max_q)\n",
    "        else:\n",
    "            for q in range(1,max_q - 1):\n",
    "                if proj_dist > proj_quantiles[q-1] and proj_dist <= proj_quantiles[q]:\n",
    "                    compare_dict[\"projected rank\"].append(q+1) \n",
    "                \n",
    "        compare_dict[\"from node id\"].append(i)\n",
    "        compare_dict[\"to node id\"].append(j)\n",
    "        compare_dict[\"from node\"].append(from_node)\n",
    "        compare_dict[\"to node\"].append(to_node)\n",
    "        compare_dict[\"hypergraph distance\"].append(hg_dist)\n",
    "        compare_dict[\"projected distance\"].append(proj_dist)\n",
    "        compare_dict[\"diff\"].append(hg_dist - proj_dist)\n",
    "        compare_dict[\"rank difference\"].append(abs(compare_dict[\"projected rank\"][-1] - compare_dict[\"hypergraph rank\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = pd.DataFrame(compare_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.sort_values(by=[\"rank difference\"],ascending=False, inplace=True)\n",
    "compare_df.to_csv(f\"compare_{data_type}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_matrix = np.zeros(shape=(max_q,max_q))\n",
    "\n",
    "N_pairs = len(compare_dict[\"hypergraph rank\"])\n",
    "\n",
    "for i in range(N_pairs):\n",
    "    h_rank = compare_dict[\"hypergraph rank\"][i]\n",
    "    g_rank = compare_dict[\"projected rank\"][i]\n",
    "    quant_matrix[h_rank - 1, g_rank - 1] += 1 / N_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(quant_matrix, cmap=\"binary\", )\n",
    "plt.xticks(range(0,max_q),labels=range(1,max_q + 1))\n",
    "plt.yticks(range(0,max_q),labels=range(1,max_q + 1))\n",
    "plt.xlabel('hypergraph distance rank')\n",
    "plt.ylabel('projected distance rank')\n",
    "plt.colorbar()\n",
    "plt.title(data_type[:4])\n",
    "plt.savefig(f'../figures/ranks_dist_{data_type}.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_name = \"q-fin.st\"\n",
    "to_name = \"q-fin.rm\"\n",
    "from_id = 65\n",
    "to_id = 67\n",
    "dists_dict[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dist_dict[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_dict[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for edge in paths_dict[from_name][to_id][0]:\n",
    "    string = ''\n",
    "    for x in edges_to_nodes_dict[edge]:\n",
    "        string += x + ' '\n",
    "    print(f'{string} weight = {edges_to_counts_dict[edge]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dist_path[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in proj_dist_path[from_name][to_id]:\n",
    "    print(f'weight = {1 / proj_graph.es[edge][\"weight\"]}')\n",
    "    print(node_names[proj_graph.es[edge].source])\n",
    "    print(node_names[proj_graph.es[edge].target])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
