{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.211701Z",
     "start_time": "2024-10-01T21:12:24.902226Z"
    }
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import pathlib\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import time\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specification of data and supplementary dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.215325Z",
     "start_time": "2024-10-01T21:12:25.212844Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_type = '2009-2010'\n",
    "data_type = '2000-2001'\n",
    "# data_type = '1999-2000'\n",
    "# data_type = 'subsample'\n",
    "DATA_PATH = f'../data arxiv/hyperedges_list_arxiv_{data_type}.txt'\n",
    "LINEGRAPH_EDGES_PATH = f'../suplementary/linegraphs/linegraph_weighted_edges_list_{data_type}.txt'\n",
    "LINEGRAPH_NODES_PATH = f'../suplementary/linegraphs/linegraph_weighted_nodes_list_{data_type}.txt'\n",
    "DIST_DIR = \"../suplementary/dist\"\n",
    "CLIQUE_PATH = f\"../suplementary/clique-projections/{data_type}-proj-graph.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.224546Z",
     "start_time": "2024-10-01T21:12:25.215868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8914\n",
      "'hep-ex', 'physics.comp-ph'\n",
      "'math.ag', 'math.cv'\n"
     ]
    }
   ],
   "source": [
    "data_path = DATA_PATH\n",
    "with open(data_path) as file:\n",
    "    line = file.readline()\n",
    "\n",
    "if data_type in [\"subsample\", \"test\"]:\n",
    "    hyperedges_list = line.split('], ')\n",
    "    hyperedges_list = [x.replace('[', '') for x in hyperedges_list]\n",
    "    hyperedges_list = [x.replace(']', '') for x in hyperedges_list]  # removes the last \"]\"\"\n",
    "else:\n",
    "    hyperedges_list = line.split(']\", ')\n",
    "    hyperedges_list = [x.replace('\"[', '') for x in hyperedges_list]\n",
    "    hyperedges_list = [x.replace(']\"', '') for x in hyperedges_list]  # removes the last \"]\"\"\n",
    "\n",
    "print(len(hyperedges_list))\n",
    "print(hyperedges_list[0])\n",
    "print(hyperedges_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.235904Z",
     "start_time": "2024-10-01T21:12:25.225386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2205\n",
      "2205\n",
      "[['astro-ph', 'astro-ph.ga'], ['astro-ph', 'cond-mat'], ['astro-ph', 'cond-mat', 'gr-qc', 'hep-ph']]\n",
      "[1 1 2]\n"
     ]
    }
   ],
   "source": [
    "hyperedges_list, counts_list = np.unique(hyperedges_list,\n",
    "                                         return_counts=True)  # counts_list contains info about frequencies of each hyperedge\n",
    "print(len(hyperedges_list))\n",
    "print(len(counts_list))\n",
    "\n",
    "hyperedges_list = [x.replace(\"'\", '') for x in hyperedges_list]\n",
    "hyperedges_list = [x.split(', ') for x in hyperedges_list]\n",
    "print(hyperedges_list[:3])\n",
    "print(counts_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.243074Z",
     "start_time": "2024-10-01T21:12:25.237729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2205\n"
     ]
    }
   ],
   "source": [
    "# left hyperedges of size 1\n",
    "hyperedges_list_ids = [i for i in range(len(hyperedges_list)) if len(hyperedges_list[i]) > 1]\n",
    "print(len(hyperedges_list_ids))\n",
    "if len(hyperedges_list_ids) < len(hyperedges_list):\n",
    "    hyperedges_list = [hyperedges_list[i] for i in hyperedges_list_ids]\n",
    "    counts_list = [counts_list[i] for i in hyperedges_list_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.246413Z",
     "start_time": "2024-10-01T21:12:25.243756Z"
    }
   },
   "outputs": [],
   "source": [
    "# create supplementary dictionary containing nodes as keys and hyperedges to which they are incident as values. It helps to access needed hyperedges faster\n",
    "def get_nodes_to_edges(edges_to_nodes_dict):\n",
    "    nodes_to_edges_dict = {}\n",
    "    for edge, nodes in edges_to_nodes_dict.items():\n",
    "        for node in nodes:\n",
    "            if node in nodes_to_edges_dict.keys():\n",
    "                nodes_to_edges_dict[node].append(edge)\n",
    "            else:\n",
    "                nodes_to_edges_dict[node] = [edge]\n",
    "    return nodes_to_edges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.250400Z",
     "start_time": "2024-10-01T21:12:25.246904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes count = 135\n"
     ]
    }
   ],
   "source": [
    "edges_to_nodes_dict = {i: hyperedges_list[i] for i in\n",
    "                       range(len(hyperedges_list))}  # key = hyperedge, value = nodes in hyperedge\n",
    "edges_to_counts_dict = {i: counts_list[i] for i in range(len(counts_list))}\n",
    "nodes_to_edges_dict = get_nodes_to_edges(\n",
    "    edges_to_nodes_dict)  # key = node, value = list of hyperedges in which the node participates\n",
    "print(f'nodes count = {len(nodes_to_edges_dict.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-graph construction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.258951Z",
     "start_time": "2024-10-01T21:12:25.251974Z"
    }
   },
   "outputs": [],
   "source": [
    "# weight_function is needed for creation of weighted line graph, w1 (w2) is e1's (e2's) weight in weighted hypergraph\n",
    "def weight_function(e1, e2, w1, w2):\n",
    "    if e1 == e2:\n",
    "        return (1 / w1) * ((1 / 3) * (len(e1) + 1) - 1)\n",
    "    union_len = len(set(e1).union(e2))\n",
    "    intersect_len = len(set(e1).intersection(e2))\n",
    "    return (1 / 2) * (1 / w1 + 1 / w2) * ((1 / 3) * union_len * (1 + 1 / intersect_len) - 1)\n",
    "\n",
    "\n",
    "def generate_weighted_line_graph(edges_to_nodes_dict, edges_to_counts_dict) -> ig.Graph:\n",
    "    file_path = pathlib.Path(LINEGRAPH_EDGES_PATH)\n",
    "\n",
    "    if not file_path.exists():\n",
    "\n",
    "        items = list(edges_to_nodes_dict.items())\n",
    "        line_nodes_ids_dict = {items[i][0]: i for i in\n",
    "                               range(len(items))}  # in line-graph nodes = hyperedges in intial hypergraph\n",
    "        line_edges_list = []\n",
    "        line_edges_weights_list = []\n",
    "\n",
    "        for i in range(len(items)):\n",
    "            edge, nodes = items[i]\n",
    "            line_edges_list.append([i, i])  # our line-graph has self-loops\n",
    "            line_edges_weights_list.append(weight_function(nodes, nodes,\n",
    "                                                           edges_to_counts_dict[edge],\n",
    "                                                           edges_to_counts_dict[edge]))  # add weights of self-loops\n",
    "\n",
    "        for i in tqdm(range(len(items))):\n",
    "            edge, nodes = items[i]\n",
    "            set_nodes = set(nodes)\n",
    "            neighbors = [k for k, v in items if\n",
    "                         not k == edge and\n",
    "                         (len(v) >= len(nodes)) and\n",
    "                         not set_nodes.isdisjoint(v)]  # find hyperedges that intersect with the current one\n",
    "\n",
    "            if len(neighbors) > 0:\n",
    "                for neighbor in neighbors:\n",
    "                    line_edges_list.append([line_nodes_ids_dict[edge], line_nodes_ids_dict[\n",
    "                        neighbor]])  # each intersection leads to edge in the line-graph\n",
    "                    line_edges_weights_list.append(weight_function(nodes, edges_to_nodes_dict[neighbor],\n",
    "                                                                   edges_to_counts_dict[edge],\n",
    "                                                                   edges_to_counts_dict[neighbor]))\n",
    "\n",
    "        G = ig.Graph(n=len(items), edges=line_edges_list,\n",
    "                     edge_attrs={'weight': line_edges_weights_list},\n",
    "                     vertex_attrs={'label': list(line_nodes_ids_dict.keys())})\n",
    "        edges_list = G.get_edgelist()\n",
    "        print('saving line graph')\n",
    "        with open(LINEGRAPH_EDGES_PATH, 'w') as file:\n",
    "            lines = []\n",
    "            for edge_id in range(len(edges_list)):\n",
    "                edge_weight = G.es[edge_id][\"weight\"]\n",
    "                lines.append(f'{edges_list[edge_id][0]} {edges_list[edge_id][1]} {edge_weight}\\n')\n",
    "            file.writelines(lines)\n",
    "        with open(LINEGRAPH_NODES_PATH, 'w') as file:\n",
    "            file.writelines([str(G.vs[i]['label']) + '\\n' for i in range(len(items))])\n",
    "\n",
    "        print('line graph saved')\n",
    "        del G\n",
    "\n",
    "    with open(LINEGRAPH_EDGES_PATH, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    with open(LINEGRAPH_NODES_PATH, 'r') as file:\n",
    "        lines_nodes = file.readlines()\n",
    "    edges_list = []\n",
    "    weights = []\n",
    "    nodes = []\n",
    "    print('preparing data for line graph creation')\n",
    "    print(len(lines))\n",
    "    for line in lines:\n",
    "        from_node, to_node, weight = line.split(' ')\n",
    "        from_node = int(from_node)\n",
    "        to_node = int(to_node)\n",
    "        if from_node not in nodes:\n",
    "            nodes.append(from_node)\n",
    "        if to_node not in nodes:\n",
    "            nodes.append(to_node)\n",
    "        weight = float(weight)\n",
    "        edges_list.append((from_node, to_node))\n",
    "        weights.append(weight)\n",
    "\n",
    "    print('line graph creation')\n",
    "    G = ig.Graph(n=len(nodes), edges=edges_list,\n",
    "                 edge_attrs={'weight': weights},\n",
    "                 vertex_attrs={'label': [int(x) for x in lines_nodes]})\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypergraph distance calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.266461Z",
     "start_time": "2024-10-01T21:12:25.259449Z"
    }
   },
   "outputs": [],
   "source": [
    "# to find shortest path from node u to node v one should, first, find path in line-graph from all nodes, representing hyperedges to which u belongs, to all nodes, representing all hyperedges to which v belongs \n",
    "# each hyperedge contains sereis of nodes, therefore,  paths between nodes in the line-graph might be the same for different node pairs.\n",
    "# To fasten calculation we can save shortest paths that we have already found  \n",
    "# global_all_path_dict contains information about previously found paths between nodes in line-graph (representing hyperedges in hypergraph)\n",
    "\n",
    "def get_distance_new(nodes_to_edges_dict, edges_to_counts_dict, line_graph : ig.Graph, u, v, global_all_path_dict={},\n",
    "                     return_path=False):\n",
    "    from_edges = nodes_to_edges_dict[u]  # from_edges - edges incident to node u\n",
    "    to_edges = nodes_to_edges_dict[v]  # to_edges - edges incident to node v \n",
    "    if len(from_edges) < len(to_edges):\n",
    "        u_edges = from_edges\n",
    "        v_edges = to_edges\n",
    "    else:\n",
    "        u_edges = to_edges\n",
    "        v_edges = from_edges\n",
    "\n",
    "    line_nodes_ids_dict = {line_graph.vs[\"label\"][i]: i for i in range(len(line_graph.vs[\"label\"]))}\n",
    "    line_ids_nodes_dict = {i: line_graph.vs[\"label\"][i] for i in range(len(line_graph.vs[\"label\"]))}\n",
    "\n",
    "    uv_dist = []\n",
    "    uv_paths = []\n",
    "    \n",
    "    for u_e in u_edges:\n",
    "        if u_e in global_all_path_dict.keys():\n",
    "            all_paths_dict = global_all_path_dict[u_e].copy()\n",
    "        else:\n",
    "            all_paths_dict = {}\n",
    "        to_linegraph_nodes = [v_e for v_e in v_edges if\n",
    "                              v_e not in all_paths_dict.keys() and not v_e == u_e]  # nodes in line-graph to which paths from u_e have not been found previously \n",
    "\n",
    "        drop_ve = []\n",
    "        for v_e in to_linegraph_nodes:\n",
    "            if v_e in global_all_path_dict.keys():  # paths are not directed. If we already have path from v_e to u_e it is the same path as from u_e to v_e and we should not find it once more\n",
    "                if u_e in global_all_path_dict[v_e].keys():\n",
    "                    all_paths_dict[v_e] = [list(np.flip(x)) for x in global_all_path_dict[v_e][u_e]]\n",
    "                    drop_ve.append(v_e)\n",
    "\n",
    "        all_paths = line_graph.get_all_shortest_paths(line_nodes_ids_dict[u_e],\n",
    "                                                      to=[line_nodes_ids_dict[x] for x in to_linegraph_nodes if\n",
    "                                                          x not in drop_ve],\n",
    "                                                      weights=line_graph.es[\n",
    "                                                          \"weight\"])  # calculation of absent shortest paths \n",
    "\n",
    "        for i in range(len(all_paths)):  # collecting of the new shortest paths\n",
    "            if len(all_paths[i]) > 1:\n",
    "                if not line_ids_nodes_dict[all_paths[i][-1]] in all_paths_dict.keys():\n",
    "                    all_paths_dict[line_ids_nodes_dict[all_paths[i][-1]]] = [all_paths[i]]\n",
    "\n",
    "                    if not line_ids_nodes_dict[all_paths[i][-1]] in global_all_path_dict.keys():\n",
    "                        global_all_path_dict[line_ids_nodes_dict[all_paths[i][-1]]] = {\n",
    "                            u_e: [list(np.flip(all_paths[i]))]}\n",
    "                    else:\n",
    "                        global_all_path_dict[line_ids_nodes_dict[all_paths[i][-1]]][u_e] = [list(np.flip(all_paths[i]))]\n",
    "                else:\n",
    "                    all_paths_dict[line_ids_nodes_dict[all_paths[i][-1]]].append(all_paths[i])\n",
    "                    global_all_path_dict[line_ids_nodes_dict[all_paths[i][-1]]][u_e].append(list(np.flip(all_paths[i])))\n",
    "\n",
    "        global_all_path_dict[\n",
    "            u_e] = all_paths_dict.copy()  # saving of the renewed collection of the shortest paths from u_e\n",
    "\n",
    "        for v_e in v_edges:  # calculate distances between u and v corresponding to every u_e -> v_e \n",
    "            if u_e == v_e:\n",
    "                dist = line_graph.es[\"weight\"][line_graph.get_eid(line_nodes_ids_dict[u_e], line_nodes_ids_dict[v_e])]\n",
    "                uv_dist.append(dist + 1 / edges_to_counts_dict[u_e])  # modification for weighted hypergraph\n",
    "                uv_paths.append([line_graph.get_eid(line_nodes_ids_dict[u_e], line_nodes_ids_dict[v_e])])\n",
    "            else:\n",
    "                paths = all_paths_dict[v_e]\n",
    "                uv_paths.append(paths)\n",
    "                if len(paths[0]) > 0:\n",
    "                    distance = 0\n",
    "                for i in range(len(paths[0]) - 1):\n",
    "                    start = paths[0][i]\n",
    "                    end = paths[0][i + 1]\n",
    "                    e = line_graph.get_eid(start, end)\n",
    "                    distance += line_graph.es[e][\"weight\"]\n",
    "                    # uv_paths[-1].append((start,end))\n",
    "                distance += (1 / 2) * (1 / edges_to_counts_dict[u_e] + 1 / edges_to_counts_dict[v_e])\n",
    "                uv_dist.append(distance)\n",
    "\n",
    "    if not return_path:\n",
    "        return min(uv_dist), global_all_path_dict\n",
    "\n",
    "    min_indexes = [i for i in range(len(uv_dist)) if uv_dist[i] == min(\n",
    "        uv_dist)]  # indexes corresponding to minimal distance between differen u_e and v_e pairs\n",
    "\n",
    "    paths = []\n",
    "    for i in min_indexes:\n",
    "        if len(uv_paths[i]) > 1:\n",
    "            paths_str = [\",\".join([str(y) for y in x]) for x in uv_paths[i]]\n",
    "            paths_str = np.unique(paths_str)\n",
    "            paths_tmp = [x.split(',') for x in paths_str]\n",
    "            paths_tmp = [[int(y) for y in x] for x in paths_tmp]\n",
    "            paths.extend(paths_tmp)\n",
    "        else:\n",
    "            paths.extend(uv_paths[i].copy())\n",
    "\n",
    "    return min(uv_dist), paths, global_all_path_dict"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import queue\n",
    "# пока требует доработок\n",
    "def get_distance_new_alg_v2(\n",
    "        nodes_from,\n",
    "        nodes_to,\n",
    "        edges_to_counts_dict,\n",
    "        line_graph : nx.Graph):\n",
    "    from_edges = set(nodes_from)  # from_edges - edges incident to node u\n",
    "    to_edges = set(nodes_to)  # to_edges - edges incident to node v \n",
    "    loops = list(from_edges.intersection(to_edges))\n",
    "    if len(loops) > 0:\n",
    "        loops_w = [line_graph[u][u]['weight'] + 1/edges_to_counts_dict[u] for u in loops]\n",
    "        min_loop = np.argmin(loops_w)\n",
    "        min_node_loop = loops[min_loop]\n",
    "        min_loop = loops_w[min_loop]\n",
    "    else:\n",
    "        min_loop = 100_000\n",
    "        min_node_loop = None\n",
    "                    \n",
    "    uv_dist = {}\n",
    "    uv_path = {}\n",
    "    for u in line_graph.nodes():\n",
    "        uv_dist[u] = float('inf')\n",
    "        uv_path[u] = None\n",
    "    \n",
    "    pq = queue.PriorityQueue()\n",
    "    for u in from_edges:\n",
    "        for v in line_graph[u]:\n",
    "            if v == u:\n",
    "                continue\n",
    "            prev_val = uv_dist[v]\n",
    "            new_val = line_graph[u][v]['weight'] + 1/2/edges_to_counts_dict[u]\n",
    "            if new_val < prev_val:\n",
    "                uv_dist[v] = new_val\n",
    "                uv_path[v] = u\n",
    "    for u in uv_dist:\n",
    "        if not np.isinf(uv_dist[u]): \n",
    "            pq.put((uv_dist[u], u))\n",
    "    min_node = None\n",
    "    min_val = float('inf')\n",
    "    while not pq.empty():\n",
    "        u = pq.get()\n",
    "        # if u[0] > min_val + 0.5:\n",
    "        #     break\n",
    "        for v in line_graph[u[1]]:\n",
    "            if v == u[1]:\n",
    "                continue\n",
    "            new_w = u[0] + line_graph[u[1]][v]['weight']\n",
    "            if uv_dist[v] > new_w:\n",
    "                uv_dist[v] = new_w\n",
    "                uv_path[v] = u[1]\n",
    "                pq.put((new_w, v))\n",
    "                if v in to_edges:\n",
    "                    w = new_w + 1/2/edges_to_counts_dict[v]\n",
    "                    if w < min_val:\n",
    "                        min_val = w\n",
    "                        min_node = v\n",
    "    \n",
    "    if min_loop < min_val:\n",
    "        return (min_loop, [min_node_loop])\n",
    "    else:\n",
    "        path = [min_node]\n",
    "        p = min_node\n",
    "        visited = set(path)\n",
    "        while uv_path[p] is not None:\n",
    "            p = uv_path[p]\n",
    "            if p in from_edges:\n",
    "                path.append(p)    \n",
    "                break\n",
    "            if p in visited:\n",
    "                break\n",
    "            visited.add(p)\n",
    "            path.append(p)\n",
    "        return (min_val, path[::-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.275899Z",
     "start_time": "2024-10-01T21:12:25.269893Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import queue\n",
    "\n",
    "# алгоритм для поиска расстояния между двумя вершинами\n",
    "# алгоритм состоит из двух частей: учет циклов e->e у учет путей, состоящих более чем из двух вершин\n",
    "def get_distance_new_alg(\n",
    "        nodes_from,\n",
    "        nodes_to,\n",
    "        edges_to_counts_dict,\n",
    "        line_graph : nx.Graph):\n",
    "    from_edges = set(nodes_from)  # from_edges - edges incident to node u\n",
    "    to_edges = set(nodes_to)  # to_edges - edges incident to node v \n",
    "    # циклы вида e->e\n",
    "    loops = list(from_edges.intersection(to_edges))\n",
    "    \n",
    "    if len(loops) > 0:\n",
    "        # ищем минимальный цикл и минимальную ноду\n",
    "        loops_w = [line_graph[u][u]['weight'] + 1/edges_to_counts_dict[u] for u in loops]\n",
    "        min_loop = np.argmin(loops_w)\n",
    "        min_node_loop = loops[min_loop]\n",
    "        min_loop = loops_w[min_loop]\n",
    "    else:\n",
    "        min_loop = 100_000\n",
    "        min_node_loop = None\n",
    "    # тут хранится расстояние до вершины, и вершина из которой быстрее всего прийти в данную вершину\n",
    "    uv_dist = {}\n",
    "    uv_path = {}\n",
    "    for u in line_graph.nodes():\n",
    "        uv_dist[u] = float('inf')\n",
    "        uv_path[u] = None\n",
    "    \n",
    "    pq = queue.PriorityQueue()\n",
    "    # сразу посещаем все вершины, до которых можно дойти из стартовых\n",
    "    # это нужно чтобы ... todo (описать случай)\n",
    "    for u in from_edges:\n",
    "        for v in line_graph[u]:\n",
    "            if v == u:\n",
    "                continue\n",
    "            prev_val = uv_dist[v]\n",
    "            new_val = line_graph[u][v]['weight'] + 1/2/edges_to_counts_dict[u]\n",
    "            if new_val < prev_val:\n",
    "                uv_dist[v] = new_val\n",
    "                uv_path[v] = u\n",
    "    \n",
    "    for u in uv_dist:\n",
    "        if not np.isinf(uv_dist[u]): \n",
    "            pq.put((uv_dist[u], u))\n",
    "    \n",
    "    # основной алгоритм\n",
    "    while not pq.empty():\n",
    "        u = pq.get()\n",
    "        for v in line_graph[u[1]]:\n",
    "            if v == u[1]:\n",
    "                continue\n",
    "            new_w = u[0] + line_graph[u[1]][v]['weight']\n",
    "            if uv_dist[v] > new_w:\n",
    "                uv_dist[v] = new_w\n",
    "                uv_path[v] = u[1]\n",
    "                pq.put((new_w, v))\n",
    "    # выбираем мин путь\n",
    "    min_node = None\n",
    "    min_val = float('inf')\n",
    "    for v in to_edges:\n",
    "        uv_dist[v]+=1/2/edges_to_counts_dict[v]\n",
    "        if min_val > uv_dist[v]:\n",
    "            min_val = uv_dist[v]\n",
    "            min_node = v\n",
    "    if min_loop < min_val:\n",
    "        return (min_loop, [min_node_loop])\n",
    "    else:\n",
    "        path = [min_node]\n",
    "        p = min_node\n",
    "        visited = set(path)\n",
    "        \n",
    "        # востановление пути todo проверить и допистаь \n",
    "        while uv_path[p] is not None:\n",
    "            p = uv_path[p]\n",
    "            if p in from_edges:\n",
    "                path.append(p)    \n",
    "                break\n",
    "            if p in visited:\n",
    "                break\n",
    "            visited.add(p)\n",
    "            path.append(p)\n",
    "        return (min_val, path[::-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.280766Z",
     "start_time": "2024-10-01T21:12:25.276574Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:25.284753Z",
     "start_time": "2024-10-01T21:12:25.281380Z"
    }
   },
   "outputs": [],
   "source": [
    "# suplementary function to calculate particulare path length\n",
    "def calc_path_weight(edges_to_counts_dict, line_graph, path):\n",
    "    line_ids_nodes_dict = {i: line_graph.vs[\"label\"][i] for i in range(len(line_graph.vs[\"label\"]))}\n",
    "    length = 0\n",
    "    for i in range(len(path) - 1):\n",
    "        start = path[i]\n",
    "        end = path[i + 1]\n",
    "        e = line_graph.get_eid(start, end)\n",
    "        length += line_graph.es[e][\"weight\"]\n",
    "    length += (1 / 2) * (1 / edges_to_counts_dict[line_ids_nodes_dict[path[0]]] + \\\n",
    "                         1 / edges_to_counts_dict[line_ids_nodes_dict[path[-1]]])\n",
    "    return length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line-graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:12:28.492744Z",
     "start_time": "2024-10-01T21:12:25.285262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data for line graph creation\n",
      "539052\n",
      "line graph creation\n"
     ]
    }
   ],
   "source": [
    "# if we constructed line-graph once it is saved into suplementary/linegraphs to fasten further calculation\n",
    "line_graph = generate_weighted_line_graph(edges_to_nodes_dict, edges_to_counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of distances between specified pairs of nodes"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "2205\n",
      "431649\n"
     ]
    }
   ],
   "source": [
    "G = line_graph.to_networkx()\n",
    "Q = nx.Graph()\n",
    "for u,du in G.nodes(data=True):\n",
    "    Q.add_node(u)\n",
    "for u in G.nodes():\n",
    "    for v in G[u]:\n",
    "        data = G[u][v]\n",
    "        Q.add_edge(u,v,weight = float(data[0]['weight']))\n",
    "del G\n",
    "print(len(Q.nodes))\n",
    "print(len(Q.edges))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T22:32:48.896492Z",
     "start_time": "2024-10-01T22:32:46.056492Z"
    }
   },
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "ss = 100**100\n",
    "sys.getsizeof(ss) / 8 / 1025"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a4b72302ddd4a239b58d1f549557647"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b4c4853778748e2b4e0b2441242fffe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "991b65aceba64cf5bb2366d4d6529a0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61c921ac268c424aba99da42b64c91dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1aecd49c998c47b3b655f44dc8ebd955"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f77557791f8741ca903ab20e96fc161a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc0b609d500f4e4e80a8ad0f75dd831e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-552:\n",
      "Process ForkPoolWorker-551:\n",
      "Exception ignored in: Process ForkPoolWorker-550:\n",
      "Process ForkPoolWorker-546:\n",
      "Process ForkPoolWorker-547:\n",
      "Process ForkPoolWorker-544:\n",
      "Process ForkPoolWorker-549:\n",
      "Process ForkPoolWorker-548:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-545:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "<bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x796896e20040>>  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user/programming/hypergraph_distances/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "      File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "def _clean_thread_parent_frames(  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      ": KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x796896e20040>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/programming/hypergraph_distances/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Process ForkPoolWorker-543:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    855\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 856\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_items\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpopleft\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    857\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[0;31mIndexError\u001B[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[84], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m data \u001B[38;5;241m=\u001B[39m get_data(s)\n\u001B[1;32m     19\u001B[0m res \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m tqdm(p\u001B[38;5;241m.\u001B[39mimap_unordered(calc, data), total\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m11\u001B[39m):\n\u001B[1;32m     21\u001B[0m     res\u001B[38;5;241m.\u001B[39mappend(r)\n\u001B[1;32m     22\u001B[0m     s\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/programming/hypergraph_distances/venv/lib/python3.10/site-packages/tqdm/notebook.py:250\u001B[0m, in \u001B[0;36mtqdm_notebook.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    249\u001B[0m     it \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__iter__\u001B[39m()\n\u001B[0;32m--> 250\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m it:\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;66;03m# return super(tqdm...) will not catch exception\u001B[39;00m\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m    253\u001B[0m \u001B[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001B[39;00m\n",
      "File \u001B[0;32m~/programming/hypergraph_distances/venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 861\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    863\u001B[0m     item \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_items\u001B[38;5;241m.\u001B[39mpopleft()\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# from threading import Semaphore\n",
    "# # from concurrent.futures.process import ProcessPoolExecutor as Pool\n",
    "# from multiprocessing import Pool\n",
    "# import numpy as np\n",
    "# def calc(data):\n",
    "#     res = np.cos(np.cos(np.cos(data[0])))\n",
    "#     return res\n",
    "# \n",
    "# def get_data(semaphore):\n",
    "#     data = (1, Q)\n",
    "#     for i in range(11):\n",
    "#         semaphore.acquire()\n",
    "#         yield data\n",
    "# \n",
    "# s = Semaphore(10000)\n",
    "# with Pool(10) as p:\n",
    "#     for i in range(1000):\n",
    "#         data = get_data(s)\n",
    "#         res = []\n",
    "#         for r in tqdm(p.imap_unordered(calc, data), total=11):\n",
    "#             res.append(r)\n",
    "#             s.release()\n",
    "# \n",
    "#         print(len(res))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T22:32:17.982106Z",
     "start_time": "2024-10-01T22:32:07.497520Z"
    }
   },
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "1000000\n",
      "1000000\n",
      "1000000\n",
      "1000000\n",
      "1000000\n",
      "1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-11:\n",
      "Process ForkPoolWorker-15:\n",
      "Process ForkPoolWorker-16:\n",
      "Process ForkPoolWorker-18:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-12:\n",
      "Exception ignored in: Process ForkPoolWorker-14:\n",
      "<bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x796896e20040>>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/user/programming/hypergraph_distances/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "      File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "def _clean_thread_parent_frames(  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      ":   File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x796896e20040>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/programming/hypergraph_distances/venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 8\u001B[0m\n\u001B[1;32m      7\u001B[0m data \u001B[38;5;241m=\u001B[39m [(j, Q) \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1000_000\u001B[39m)]\n\u001B[0;32m----> 8\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcalc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(res))\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001B[0m, in \u001B[0;36mPool.map\u001B[0;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;124;03mApply `func` to each element in `iterable`, collecting the results\u001B[39;00m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;124;03min a list that is returned.\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[0;32m--> 367\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 768\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mready():\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001B[0m, in \u001B[0;36mApplyResult.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:607\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 607\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m     \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m     gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalc\u001B[39m(data):\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mcos(np\u001B[38;5;241m.\u001B[39mcos(np\u001B[38;5;241m.\u001B[39mcos(data[\u001B[38;5;241m0\u001B[39m])))\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Pool(\u001B[38;5;241m10\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1000\u001B[39m):\n\u001B[1;32m      7\u001B[0m         data \u001B[38;5;241m=\u001B[39m [(j, Q) \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1000_000\u001B[39m)]\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:739\u001B[0m, in \u001B[0;36mPool.__exit__\u001B[0;34m(self, exc_type, exc_val, exc_tb)\u001B[0m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, exc_type, exc_val, exc_tb):\n\u001B[0;32m--> 739\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mterminate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:657\u001B[0m, in \u001B[0;36mPool.terminate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    655\u001B[0m util\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mterminating pool\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m=\u001B[39m TERMINATE\n\u001B[0;32m--> 657\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_terminate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/util.py:224\u001B[0m, in \u001B[0;36mFinalize.__call__\u001B[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001B[0m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    222\u001B[0m     sub_debug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfinalizer calling \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m with args \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m and kwargs \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m    223\u001B[0m               \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_kwargs)\n\u001B[0;32m--> 224\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_callback\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_weakref \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_args \u001B[38;5;241m=\u001B[39m \\\n\u001B[1;32m    226\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:720\u001B[0m, in \u001B[0;36mPool._terminate_pool\u001B[0;34m(cls, taskqueue, inqueue, outqueue, pool, change_notifier, worker_handler, task_handler, result_handler, cache)\u001B[0m\n\u001B[1;32m    718\u001B[0m util\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjoining task handler\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m threading\u001B[38;5;241m.\u001B[39mcurrent_thread() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m task_handler:\n\u001B[0;32m--> 720\u001B[0m     \u001B[43mtask_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    722\u001B[0m util\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjoining result handler\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    723\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m threading\u001B[38;5;241m.\u001B[39mcurrent_thread() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result_handler:\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:1096\u001B[0m, in \u001B[0;36mThread.join\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1093\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot join current thread\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1095\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1096\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_wait_for_tstate_lock\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1098\u001B[0m     \u001B[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001B[39;00m\n\u001B[1;32m   1099\u001B[0m     \u001B[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001B[39;00m\n\u001B[1;32m   1100\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait_for_tstate_lock(timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mmax\u001B[39m(timeout, \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:1116\u001B[0m, in \u001B[0;36mThread._wait_for_tstate_lock\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m   1113\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m   1115\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1116\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1117\u001B[0m         lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m   1118\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stop()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# from multiprocessing import Pool\n",
    "# import numpy as np\n",
    "# def calc(data):\n",
    "#     return np.cos(np.cos(np.cos(data[0])))\n",
    "# with Pool(10) as p:\n",
    "#     for i in range(1000):\n",
    "#         data = [(j, Q) for j in range(1000_000)]\n",
    "#         res = p.map(calc, data)\n",
    "#         print(len(res))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T21:20:46.349603Z",
     "start_time": "2024-10-01T21:12:31.301084Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T21:58:03.023566Z",
     "start_time": "2024-10-01T21:58:03.019974Z"
    }
   },
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "from threading import Semaphore\n",
    "from concurrent.futures.process import ProcessPoolExecutor as Pool\n",
    "dists_dict = {}\n",
    "paths_dict = {}\n",
    "node_names = list(nodes_to_edges_dict.keys())\n",
    "start_id = 0\n",
    "final_id = len(node_names) - 1\n",
    "# start_id = 120\n",
    "# final_id = 120\n",
    "\n",
    "\n",
    "def do_calc(data):\n",
    "    nodes_to_edges_dict, edges_to_counts_dict, Q, nodes_from, us, i = data\n",
    "    result = []\n",
    "    print('start:', i)\n",
    "    for node_from in tqdm(nodes_from, position=i):\n",
    "        for i, u in enumerate(us):\n",
    "            # print(u)\n",
    "            if u == node_from:\n",
    "                result.append((node_from, u,0))\n",
    "                continue\n",
    "            data, path = get_distance_new_alg(nodes_to_edges_dict[node_from], nodes_to_edges_dict[u], edges_to_counts_dict,Q)\n",
    "            # old = old_res['astro-ph'][i]\n",
    "            # delta = old - data\n",
    "            result.append((node_from, u, data))\n",
    "            # print(my_round(delta), path)\n",
    "    return result\n",
    "\n",
    "WORKERS = 10\n",
    "to_points = list(nodes_to_edges_dict.keys())\n",
    "# with Pool(WORKERS) as p:\n",
    "#             \n",
    "#     for id_from in trange(start_id, final_id + 1,desc='find_paths', position = 1):\n",
    "#         time_start = time.time()\n",
    "#         # if id_from == 3:\n",
    "#         #     break\n",
    "#         node_from = node_names[id_from]\n",
    "#         dists = []\n",
    "#         paths = []\n",
    "#         print(id_from, node_from)\n",
    "#         data= [(nodes_to_edges_dict, edges_to_counts_dict, Q, node_from, [u for u in to_points[i::WORKERS]], i) for i in range(WORKERS)]\n",
    "#         \n",
    "#         data = list(tqdm(p.imap_unordered(do_calc, data), total=len(data)))\n",
    "#         for dd in data:\n",
    "#             for d in dd:\n",
    "#                 if d[0] not in dists_dict:\n",
    "#                     dists_dict[d[0]] = {}\n",
    "#                 dists_dict[d[0]][d[1]] = d[2]\n",
    "#         with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle',\n",
    "#                   'wb') as handle:  # we save calculated distances and corresponding shortest paths to fasten furter analysis\n",
    "#             pickle.dump((dists_dict, paths_dict), handle)\n",
    "#         time_term = time.time()\n",
    "#         print(f'Elapsed time {time_term - time_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "многопоточная реализация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78e69b9f3f314bd999d4c2d5acefc81b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9643044dbc144fdf9f31f8b489f14cfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c53e27d8139497391498d3fcecf660e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb5d2a4fd89746568f92018053b3810d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fd5ded9987914aa4b88c4680ac502f0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "694b500a86ac48999b409f797904c34f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f54a919cb7641168139cdbda6787cf3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "541aa0bffa6742c89ac42c174a4ef272"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e1eab83472164e38a8ce273e97f8cd79"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3906ce8d07b24b358c7589d3ea60de62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b87f72c2dda24bafa53105fcc9d8237f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "573e07ef6d4a45f68b52586e85ea9bad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 12\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e397ea4aab324236906653d632bfaf59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58689c11a0304e628b36dbc9171efbaa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b2027f146bd4f5788a0593ab8193f35"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e8d7e020a624387b08d02bdb55a65a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a949c01b75745b99adc0cd735a70897"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 17\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d0aab8a49059460bbeadec91f0276cae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e3aa1f23fbe43ac8f01a4fee6374e0c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20a10f180bd14b28b2b1d4f10976154d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[79], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m time_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Pool(WORKERS) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[0;32m----> 7\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdo_calc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m res \u001B[38;5;241m=\u001B[39m [r \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m res]\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDIST_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/dists_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_type\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_weighted-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfinal_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     10\u001B[0m                   \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m handle:  \u001B[38;5;66;03m# we save calculated distances and corresponding shortest paths to fasten furter analysis\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:367\u001B[0m, in \u001B[0;36mPool.map\u001B[0;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, iterable, chunksize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    363\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001B[39;00m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;124;03m    in a list that is returned.\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[0;32m--> 367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:768\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 768\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mready():\n\u001B[1;32m    770\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:765\u001B[0m, in \u001B[0;36mApplyResult.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:607\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    605\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 607\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "to_points = list(nodes_to_edges_dict.keys())\n",
    "WORKERS = 20\n",
    "# данные для расчета на нескольких потоках \n",
    "# каждому потоку дается список вида (nodes_to_edges_dict, edges_to_counts_dict, line_graph, ноды от которых надо искать пути данному потоку, все ноды до которых надо дойти)\n",
    "data = [(nodes_to_edges_dict, edges_to_counts_dict, Q, [node_names[id_from] for id_from in range(start_id+i, final_id+1,WORKERS)], [u for u in to_points], i) for i in range(WORKERS)]\n",
    "time_start = time.time()\n",
    "\n",
    "with Pool(WORKERS) as p:\n",
    "    # res это список словарей вида {node_from: {node_to: length}}\n",
    "    res = p.map(do_calc, data)\n",
    "res = [r for r in res]\n",
    "with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle',\n",
    "                  'wb') as handle:  # we save calculated distances and corresponding shortest paths to fasten furter analysis\n",
    "            pickle.dump(res, handle)\n",
    "time_term = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T22:30:05.605767Z",
     "start_time": "2024-10-01T22:08:33.834238Z"
    }
   },
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f'Elapsed time {time_term - time_start}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T22:30:05.607092Z",
     "start_time": "2024-10-01T22:30:05.607006Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init astro-ph\n",
      "init astro-ph.ga\n",
      "init cond-mat\n",
      "init gr-qc\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 5)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRemoteTraceback\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;31mRemoteTraceback\u001B[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/tmp/ipykernel_6809/927226784.py\", line 14, in do_calc\n    nodes_to_edges_dict, edges_to_counts_dict, Q, nodes_from, us, i = data\nValueError: not enough values to unpack (expected 6, got 5)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m (nodes_to_edges_dict, edges_to_counts_dict, Q, node_from, [u])\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Pool(WORKERS) \u001B[38;5;28;01mas\u001B[39;00m p:\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m p\u001B[38;5;241m.\u001B[39mimap_unordered(do_calc, get_data()):\n\u001B[1;32m     11\u001B[0m         \u001B[38;5;28mprint\u001B[39m(r)\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/pool.py:873\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    871\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[1;32m    872\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n\u001B[0;32m--> 873\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m value\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 6, got 5)"
     ]
    }
   ],
   "source": [
    "# from multiprocessing import Pool\n",
    "# to_points = list(nodes_to_edges_dict.keys())\n",
    "# def get_data():\n",
    "#     for id_from in range(start_id, final_id + 1):\n",
    "#         node_from = node_names[id_from]\n",
    "#         for u in to_points:\n",
    "#             print(f'init {u}')\n",
    "#             yield (nodes_to_edges_dict, edges_to_counts_dict, Q, node_from, [u])\n",
    "# with Pool(WORKERS) as p:\n",
    "#     for r in p.imap_unordered(do_calc, get_data()):\n",
    "#         print(r)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-01T21:57:36.362554Z",
     "start_time": "2024-10-01T21:57:35.256302Z"
    }
   },
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "to_points = list(nodes_to_edges_dict.keys())\n",
    "new_res = {k:[dists_dict[k][i] for i in to_points] for k in dists_dict}\n",
    "new_res.keys()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "idx=[40, 897, 999]\n",
    "for i in idx:\n",
    "    name = ''\n",
    "    for x in edges_to_nodes_dict[i]:\n",
    "        name+=x + ' '\n",
    "    print(f'{name} w = {edges_to_counts_dict[i]}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# -0.003700000000000002 0.053897806154750594 astro-ph math-ph\n",
    "# -0.003700000000000002 0.053897806154750594 astro-ph math.mp"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(f'../data/dists_2000-2001_weighted-0-134.pickle', 'rb') as handle:\n",
    "    old_res, paths = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "old_res['astro-ph']"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def my_round(x):\n",
    "    r = 10000\n",
    "    return int(r*x)/r"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for f in new_res:\n",
    "    o = old_res[f]\n",
    "    n = new_res[f]\n",
    "    for i in range(len(o)):\n",
    "        delta = my_round(o[i]) - my_round(n[i])\n",
    "        if abs(delta)!=0:\n",
    "            print(f, new_res[f][i], delta)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "s = 0\n",
    "e = 0\n",
    "for i in range(len(old_res['astro-ph'])):\n",
    "    delta = my_round(old_res['astro-ph'][i]) - my_round(dists_dict['astro-ph'][i])\n",
    "    if abs(delta)!=0:\n",
    "        print(old_res['astro-ph'][i], dists_dict['astro-ph'][i],delta/old_res['astro-ph'][i] * 100)\n",
    "        e+=1\n",
    "    else:\n",
    "        s+=1"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(e/(s+e)*100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dists_dict = {}\n",
    "paths_dict = {}\n",
    "node_names = list(nodes_to_edges_dict.keys())\n",
    "start_id = 0\n",
    "final_id = len(node_names) - 1\n",
    "dist, paths_tmp, global_all_path_dict = get_distance_new(nodes_to_edges_dict, edges_to_counts_dict, line_graph, 'astro-ph', 'math-ph', global_all_path_dict, return_path=True)\n",
    "dist"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dists_dict = {}\n",
    "paths_dict = {}\n",
    "node_names = list(nodes_to_edges_dict.keys())\n",
    "start_id = 0\n",
    "final_id = len(node_names) - 1\n",
    "\n",
    "# start_id = 120\n",
    "# final_id = 120\n",
    "file_path = pathlib.Path(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle')\n",
    "\n",
    "global_all_path_dict = {}\n",
    "    \n",
    "if True:\n",
    "    for id_from in trange(start_id, final_id + 1, position=1):\n",
    "        time_start = time.time()\n",
    "    \n",
    "        node_from = node_names[id_from]\n",
    "        dists = []\n",
    "        paths = []\n",
    "        print('id:', id_from)\n",
    "        for node in tqdm(nodes_to_edges_dict.keys(), position=2):\n",
    "            if not node == node_from:\n",
    "                dist, paths_tmp, global_all_path_dict = get_distance_new(nodes_to_edges_dict, edges_to_counts_dict, line_graph, node_from, node, global_all_path_dict, return_path=True)\n",
    "                dists.append(dist)\n",
    "                paths.append(paths_tmp)\n",
    "            else:\n",
    "                dists.append(0)\n",
    "                paths.append([])\n",
    "        dists_dict[node_from] = dists.copy()\n",
    "        paths_dict[node_from] = paths.copy()\n",
    "        with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle', 'wb') as handle: # we save calculated distances and corresponding shortest paths to fasten furter analysis\n",
    "                pickle.dump((dists_dict, paths_dict), handle)\n",
    "        time_term = time.time()\n",
    "        print(f'Elapsed time {time_term - time_start}')\n",
    "    \n",
    "else:\n",
    "    with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle', 'rb') as handle:\n",
    "        dists_dict, paths_dict = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "dists_dict = {}\n",
    "paths_dict = {}\n",
    "node_names = list(nodes_to_edges_dict.keys())\n",
    "start_id = 0\n",
    "final_id = len(node_names) - 1\n",
    "# start_id = 120\n",
    "# final_id = 120\n",
    "file_path = pathlib.Path(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle')\n",
    "\n",
    "global_all_path_dict = {}\n",
    "\n",
    "if not file_path.exists():\n",
    "    time_start = time.time()\n",
    "    for id_from in trange(start_id, final_id + 1, position=1):\n",
    "        node_from = node_names[id_from]\n",
    "        dists = []\n",
    "        paths = []\n",
    "        print(id_from, node_from)\n",
    "        for node in tqdm(nodes_to_edges_dict.keys(), position=2):\n",
    "            if not node == node_from:\n",
    "                dist = get_distance_new(nodes_to_edges_dict, edges_to_counts_dict,\n",
    "                                                                         li, node_from, node)\n",
    "                dists.append(dist)\n",
    "            else:\n",
    "                dists.append(0)\n",
    "                paths.append([])\n",
    "        dists_dict[node_from] = dists.copy()\n",
    "        paths_dict[node_from] = paths.copy()\n",
    "        with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle',\n",
    "                  'wb') as handle:  # we save calculated distances and corresponding shortest paths to fasten furter analysis\n",
    "            pickle.dump((dists_dict, paths_dict), handle)\n",
    "    time_term = time.time()\n",
    "    print(f'Elapsed time {time_term - time_start}')\n",
    "else:\n",
    "    with open(f'{DIST_DIR}/dists_{data_type}_weighted-{start_id}-{final_id}.pickle', 'rb') as handle:\n",
    "        dists_dict, paths_dict = pickle.load(handle)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame containing calculated distances between every nodes pair\n",
    "hyper_dist_df = pd.DataFrame(dists_dict)\n",
    "hyper_dist_df.index = list(nodes_to_edges_dict.keys())\n",
    "hyper_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted clique projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generating weighted clique projection\n",
    "# weights = 1 / frequency of appearance\n",
    "def make_clique_projection_data(edges_to_nodes_dict, edges_to_counts_dict):\n",
    "    file_path = pathlib.Path(CLIQUE_PATH)\n",
    "\n",
    "    if not file_path.exists():\n",
    "        proj_edges_dict = {}\n",
    "        for edge, nodes in edges_to_nodes_dict.items():\n",
    "            for i in range(len(nodes) - 1):\n",
    "                for j in range(i + 1, len(nodes)):\n",
    "                    node_from = nodes[i]\n",
    "                    node_to = nodes[j]\n",
    "                    if not (((node_from, node_to) in proj_edges_dict.keys())\n",
    "                            or ((node_to, node_from) in proj_edges_dict.keys())):\n",
    "                        proj_edges_dict[(node_from, node_to)] = edges_to_counts_dict[edge]\n",
    "                    else:\n",
    "                        if ((node_from, node_to) in proj_edges_dict.keys()):\n",
    "                            proj_edges_dict[(node_from, node_to)] += edges_to_counts_dict[edge]\n",
    "                        else:\n",
    "                            proj_edges_dict[(node_to, node_from)] += edges_to_counts_dict[edge]\n",
    "\n",
    "        with open(CLIQUE_PATH, 'w') as handle:\n",
    "            for (node_from, node_to), weight in proj_edges_dict.items():\n",
    "                handle.write(f'{node_from} {node_to} {weight}')\n",
    "                handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, create cluque projection if it wasn't created previously\n",
    "make_clique_projection_data(edges_to_nodes_dict, edges_to_counts_dict)\n",
    "\n",
    "# load and clique projection\n",
    "proj_graph_file = CLIQUE_PATH\n",
    "with open(proj_graph_file) as file:\n",
    "    lines = file.readlines()\n",
    "    edges_list = []\n",
    "    weights_list = []\n",
    "    nodes_dict = {}\n",
    "    i = -1\n",
    "    for line in lines:\n",
    "        from_node, to_node, weight = line.split(\" \")\n",
    "\n",
    "        # from_node = int(from_node)\n",
    "        # to_node = int(to_node)\n",
    "\n",
    "        if not from_node in nodes_dict.keys():\n",
    "            i += 1\n",
    "            from_id = i\n",
    "            nodes_dict[from_node] = i\n",
    "        else:\n",
    "            from_id = nodes_dict[from_node]\n",
    "        if not to_node in nodes_dict.keys():\n",
    "            i += 1\n",
    "            to_id = i\n",
    "            nodes_dict[to_node] = i\n",
    "        else:\n",
    "            to_id = nodes_dict[to_node]\n",
    "        edges_list.append([from_id, to_id])\n",
    "        weights_list.append(1 / float(weight))\n",
    "\n",
    "proj_graph = ig.Graph(edges_list)\n",
    "proj_graph.es[\"weight\"] = weights_list\n",
    "\n",
    "proj_dist_dict = {}\n",
    "proj_dist_path = {}\n",
    "\n",
    "# calculation of distances and paths in cluque projection\n",
    "\n",
    "for from_node in dists_dict.keys():\n",
    "    from_node_id = nodes_dict[from_node]\n",
    "    dists = []\n",
    "    paths_tmp = []\n",
    "    for node in list(nodes_to_edges_dict.keys()):\n",
    "        node_id = nodes_dict[node]\n",
    "        paths = proj_graph.get_shortest_paths(from_node_id, node_id, weights=proj_graph.es[\"weight\"], output=\"epath\")\n",
    "        paths_tmp.append(paths[0])\n",
    "        distance = 0\n",
    "        for e in paths[0]:\n",
    "            distance += proj_graph.es[e][\"weight\"]\n",
    "        dists.append(distance)\n",
    "\n",
    "    proj_dist_dict[from_node] = dists\n",
    "    proj_dist_path[from_node] = paths_tmp\n",
    "\n",
    "# data frame containing dists in clique proj for every nodes pair\n",
    "proj_dist_df = pd.DataFrame(proj_dist_dict)\n",
    "proj_dist_df.index = list(nodes_to_edges_dict.keys())\n",
    "proj_dist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_dist_df.to_csv(f\"results_hypergraph_dist_{data_type}.csv\")\n",
    "proj_dist_df.to_csv(f\"results_clique-proj_dist_{data_type}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hg_dists = []\n",
    "all_proj_dists = []\n",
    "\n",
    "for i in range(len(node_names) - 1):\n",
    "    for j in range(i + 1, len(node_names)):\n",
    "        from_node = node_names[i]\n",
    "        to_node = node_names[j]\n",
    "        if not from_node == to_node:\n",
    "            all_hg_dists.append(dists_dict[from_node][j])\n",
    "            all_proj_dists.append(proj_dist_dict[from_node][j])\n",
    "max_q = 10\n",
    "hg_quantiles = [np.quantile(all_hg_dists, q / max_q) for q in range(1, max_q)]\n",
    "proj_quantiles = [np.quantile(all_proj_dists, q / max_q) for q in range(1, max_q)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dict = {\n",
    "    \"from node id\": [],\n",
    "    \"to node id\": [],\n",
    "    \"from node\": [],\n",
    "    \"to node\": [],\n",
    "    \"hypergraph distance\": [],\n",
    "    \"projected distance\": [],\n",
    "    \"diff\": [],\n",
    "    \"hypergraph rank\": [],\n",
    "    \"projected rank\": [],\n",
    "    \"rank difference\": []\n",
    "}\n",
    "for i in range(len(node_names) - 1):\n",
    "    for j in range(i + 1, len(node_names)):\n",
    "        from_node = node_names[i]\n",
    "        to_node = node_names[j]\n",
    "        hg_dist = dists_dict[from_node][j]\n",
    "        proj_dist = proj_dist_dict[from_node][j]\n",
    "\n",
    "        if hg_dist <= hg_quantiles[0]:\n",
    "            compare_dict[\"hypergraph rank\"].append(1)\n",
    "        elif hg_dist > hg_quantiles[-1]:\n",
    "            compare_dict[\"hypergraph rank\"].append(max_q)\n",
    "        else:\n",
    "            for q in range(1, max_q - 1):\n",
    "                if hg_dist > hg_quantiles[q - 1] and hg_dist <= hg_quantiles[q]:\n",
    "                    compare_dict[\"hypergraph rank\"].append(q + 1)\n",
    "\n",
    "        if proj_dist <= proj_quantiles[0]:\n",
    "            compare_dict[\"projected rank\"].append(1)\n",
    "        elif proj_dist > proj_quantiles[-1]:\n",
    "            compare_dict[\"projected rank\"].append(max_q)\n",
    "        else:\n",
    "            for q in range(1, max_q - 1):\n",
    "                if proj_dist > proj_quantiles[q - 1] and proj_dist <= proj_quantiles[q]:\n",
    "                    compare_dict[\"projected rank\"].append(q + 1)\n",
    "\n",
    "        compare_dict[\"from node id\"].append(i)\n",
    "        compare_dict[\"to node id\"].append(j)\n",
    "        compare_dict[\"from node\"].append(from_node)\n",
    "        compare_dict[\"to node\"].append(to_node)\n",
    "        compare_dict[\"hypergraph distance\"].append(hg_dist)\n",
    "        compare_dict[\"projected distance\"].append(proj_dist)\n",
    "        compare_dict[\"diff\"].append(hg_dist - proj_dist)\n",
    "        compare_dict[\"rank difference\"].append(\n",
    "            abs(compare_dict[\"projected rank\"][-1] - compare_dict[\"hypergraph rank\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df = pd.DataFrame(compare_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df.sort_values(by=[\"rank difference\"], ascending=False, inplace=True)\n",
    "compare_df.to_csv(f\"compare_{data_type}.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_matrix = np.zeros(shape=(max_q, max_q))\n",
    "\n",
    "N_pairs = len(compare_dict[\"hypergraph rank\"])\n",
    "\n",
    "for i in range(N_pairs):\n",
    "    h_rank = compare_dict[\"hypergraph rank\"][i]\n",
    "    g_rank = compare_dict[\"projected rank\"][i]\n",
    "    quant_matrix[h_rank - 1, g_rank - 1] += 1 / N_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(quant_matrix, cmap=\"binary\", )\n",
    "plt.xticks(range(0, max_q), labels=range(1, max_q + 1))\n",
    "plt.yticks(range(0, max_q), labels=range(1, max_q + 1))\n",
    "plt.xlabel('hypergraph distance rank')\n",
    "plt.ylabel('projected distance rank')\n",
    "plt.colorbar()\n",
    "plt.title(data_type[:4])\n",
    "plt.savefig(f'../figures/ranks_dist_{data_type}.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_name = \"q-fin.st\"\n",
    "to_name = \"q-fin.rm\"\n",
    "from_id = 65\n",
    "to_id = 67\n",
    "dists_dict[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dist_dict[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_dict[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for edge in paths_dict[from_name][to_id][0]:\n",
    "    string = ''\n",
    "    for x in edges_to_nodes_dict[edge]:\n",
    "        string += x + ' '\n",
    "    print(f'{string} weight = {edges_to_counts_dict[edge]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dist_path[from_name][to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in proj_dist_path[from_name][to_id]:\n",
    "    print(f'weight = {1 / proj_graph.es[edge][\"weight\"]}')\n",
    "    print(node_names[proj_graph.es[edge].source])\n",
    "    print(node_names[proj_graph.es[edge].target])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
